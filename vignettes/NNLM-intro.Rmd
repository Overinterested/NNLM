---
title: "A quick guide to NNLM"
author: "Eric Xihui Lin"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Description

This package includes two main functions, `nnls` and `nnmf`.  `nnls` solves the following non-negative least square(NNLS)
$$ \mbox{argmin}_{\beta \ge 0} ||y - x\beta||_F $$
where $F$ is the Frobenius normal of a matrix, analogous to the L2 normal of a vector. While `nnmf` solves a non-negative
matrix factorization problem like
$$ \mbox{argmin}_{W \ge 0, H \ge 0} ||A - WH||_F^2 + \eta ||W||_F^2 + \beta \sum_{j=1}^m ||h_j||_1^2 $$
where $m$ is the number of columns of $A$, $h_j$ is the j-th column of $H$. Here $\eta$ can used to 
control magnitude of $W$ and $\beta$ is for both magnitude and sparsity of matrix $H$.


The function `nnls` in R package [_nnls_](https://cran.r-project.org/web/packages/nnls/index.html)
implemented Lawson-Hanson algorithm in Fortran for the above NNLS problem.
However the Lawson-Hanson algorithm is too slow to be embedded to solve other problems like NMF.
In this package, `nnls` is implemented in C++, using a coordinate-wise descent algorithm,
which has been shown to be much faster.  `nnmf` is a non-negative matrix factorization solver
using alternating NNLS and Brunet's multiplicative updates,
which are both implemented in C++ too. Due to the fast `nnls`, `nnmf` is way faster
than the standard R package [_NMF_](https://cran.r-project.org/web/packages/NMF/index.html). 
Thus _NNLM_ is a package more suitable for larger data sets and bigger hidden features (rank).  
In addition. `nnls` is parallelled via openMP for even better performance.


## Install

```{r, echo = TRUE, eval = FALSE}
# from CRAN (not available yet)
# install.packages('NNLM');

# or get a dev-version
library(devtools);
install_github('linxihui/NNLM')
```


## Applications

NMF has been widely used in many area to discover hidden features.

### Text mining

In text mining, such as topic discovery, NMF is used to group documents into topics, where
$A$ is a bag-of-word representation (count of a word in a document) of a batch of documents,
with _rows = vocabulary_, and _columns = documents_.  In NMF, the columns of $W$ (after normalization) represent topics (a topic
is represented as the distribution over vocabulary), and a column of $H$ shows the topics a document covered. This is similar
to latent Dirichlet allocation (LDA) analysis, except that NMF cannot guarantee that all the columns of $H$ sum up to 1 
(probability distribution).

### Meta-genes

In bioinformatics, NMF can be used to discover 'meta-genes' from expression profile, which are linear combinations of genes that
may or may not related to some biology path ways.  In this case, $A$ is usually arranged as _rows = genes_ and _columns = patients_.
The columns of $W$ can then be interpreted as meta-genes, and $H$ are said to be the expression profile of meta-genes.

### Tumour content deconvolution

In micro-array data, the mRNA profile (tumour profile) is typically a mixture of 
cancer specific profile and healthy profile. In NMF, it can be viewed as
$$ A = W H + W_0 H_1,$$
where $W$ is unknown cancer profile, and $W_0$ is known healthy profile. The task here is
to de-convolute $W$, $H$ and $H_1$ from $A$ and $W_0$. 


A more general deconvolution task can be expressed as
$$ A = W H + W_0 H_1 + W_1 H_0,$$
where $H_0$ is known coefficient matrix, e.g. a column matrix of 1. In this scenario,
$W_1$ can be interpreted as _homogeneous_ cancer profile within the specific cancer patients,
and $W$ is _heterogeneous_ cancer profile of interest for downstream analysis, such as
diagnostic or prognostic capacity, sub-type clustering.


This general deconvolution is implemented in `nnmf` via the alternating NNLS algorithm. 
The known profile $W_0$ and $H_0$ can be passed via arguments `W0` and `H0`. $L_2$ and $L_1$
constrain for unknown matrices are also supported.


### Sub-network integrated NNMF

Assume $S = {s_1, ..., s_L}$, where $s_l, \, l = 1, ..., L$ is a set of genes in sub-network $s_l$. One can
design $W$ to be a matrix of $l$ columns (or more), with $W_{i, l} = 0, \, i \not\in s_l$.  Then the matrix
factorization would learn the expression profile $W_{i, l}, \, i\in S_l$ from the data. This is implemented 
in `nnmf` with a logical mask matrix `Wm` = $\{\delta_{i\in s_l, l}\}$.


### Missing values imputation and application in recommendation system

Since matrix $A$ is assumed to have a low rank $k$ decomposition, information in $A$ is redundant.
Thus it is possible to allow some entries in $A$ to be absent.  One can use the non-missing entries to compute
$W$ and $H$. Such methodology can be used to imputation the missing entries in $A$. One application would be
recommendation system. For example, on Netflix, each customer scores only a small proportion of
all the movies on Netflix and each movie is scored by a fraction of customers. 
Thus the movie-customer comments (scores) matrix are fairly spares (lots of missings). 
Using a NNMF with missing values, one can predict a customer's scores on movies he/she has not watched. 
A recommendation can be done simply based on the predicted scores.  In addition, the resulting $W$ and $H$ 
can be used to further cluster movies and customers. This method is also implemented in `nnmf` when the input 
matrix `A` includes missing values.


### Noise reduction

This is obvious as the reconstruction from $W$ and $H$ lies on a smaller dimension, and should therefore
give a smoother reconstruction. This noise reduction is particularly useful when the noise is not Gaussian
which cannot be done using many other methods where Gaussian noise is assumed.



## Example 1: Non-small Cell Lung Cancer expression data

```{r nsclc, fig.show = 'hold', message = TRUE}
library(NNLM);

data(nsclc, package = 'NNLM')
str(nsclc)

# create 5 meta-gene signatures, using only 1 thread (no parallel)
decomp <- nnmf(nsclc[, 1:80], 5, method = 'nnls', n.threads = 1, rel.tol = 1e-6)
decomp

plot(decomp, 'W', xlab = 'Meta-gene', ylab = 'Gene')
plot(decomp, 'H', ylab = 'Meta-gene', xlab = 'Patient')
```

```{r nsclc2, fig.show = 'hold', message = TRUE, fig.align = 'center'}
plot(decomp, ylab = 'RMSE')
```

We see that the default alternating NNLS method coverage fairly quickly.

```{r nsclc3, fig.show = 'hold', message = TRUE, fig.align = 'center'}
# find the expressions of meta-genes for patient 81-100
newH <- predict(decomp, nsclc[, 81:100], which = 'H', show.progress = FALSE)
str(newH)
```

## Example 2: simulated deconvolution

```{r deconvol, message = TRUE}
# set up matrix
n <- 1000; m <- 200;
k <- 5; k1 <- 2; k2 <- 1;

set.seed(123);
W <- matrix(runif(n*k), n, k); # unknown heterogeneous cancer profile
H <- matrix(runif(k*m), k, m);
W0 <- matrix(runif(n*k1), n, k1); # known healthy profile
H1 <- matrix(runif(k1*m), k1, m);
W1 <- matrix(runif(n*k2), n, k2); # unknown common cancer profile
H0 <- matrix(1, k2, m);
noise <- 0.01*matrix(runif(n*m), n, m);

# A is the observed profile to be de-convoluted
A <- W %*% H + W0 %*% H1 + W1 %*% H0 + noise;

deconvol <- nnmf(A, k = 5, W0 = W0, H0 = H0);
```

Check if $W$ and $H$, our main interest, are recovered.

```{r deconvol2, message = TRUE}
round(cor(W, deconvol$W), 2);
round(cor(t(H), t(deconvol$H)), 2);
```

We see that $W$, $H$ are just permuted. However, as we known that
the minimization problem for NMF usually has not unique solutions
for $W$ and $H$. Therefore, $W$ and $H$ cannot be guaranteed to
be recovered exactly(different only with a permutation and a scaling).

```{r deconvol3, message = TRUE}
permutation <- c(3, 1, 5, 2, 4);
round(cor(W, deconvol$W[, permutation]), 2);
round(cor(t(H), t(deconvol$H[permutation, ])), 2);
```


As from the following result, $H_1$, coefficients of health profile and
$W_1$, common cancer profile, are recovered fairly well.

```{r deconvol4, message = TRUE}
round(cor(t(H1)), 2);
round(cor(t(H1), t(deconvol$H1)), 2);

round(cor(W1, deconvol$W1), 2);
```
